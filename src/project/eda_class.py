# -*- coding: utf-8 -*-
"""eda_class.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YcrJAPxPFsvBhJly8g5wcv-QR96e9XiF
"""

# =============================================================================
# M0 - Exploratory Data Analysis (EDA)
# =============================================================================
# Group Number: 14
# Student Names: Fanica Pedro, Fruttidoro Andrea, Weiss Quentin
# =============================================================================

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder
from sklearn.impute import KNNImputer
from scipy import stats
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
import warnings
warnings.filterwarnings('ignore')

# Set consistent style for all visualizations
plt.style.use('default')
sns.set_palette("husl")


class AutomatedEDA:
    """
    Comprehensive automated exploratory data analysis and preprocessing class.

    This class provides a complete toolkit for EDA including:
    - Automatic column type detection (numeric, binary, categorical)
    - Missing value handling (multiple strategies)
    - Data scaling and transformation
    - Outlier detection and removal
    - Normality testing
    - Encoding (ordinal, one-hot, binary)
    - Class imbalance analysis and resampling
    - Comprehensive visualization utilities

    Attributes
    ----------
    df : pd.DataFrame
        The working dataframe (copy of original)
    df_original : pd.DataFrame
        Preserved original dataframe for reference
    numeric_cols : list
        Detected numeric columns (excluding binary)
    categorical_cols : list
        Detected categorical columns (>2 unique values)
    binary_cols : list
        Detected binary columns (0/1, bool, or 2 unique values)
    preprocessing_log : list
        Log of all preprocessing steps applied

    Examples
    --------
    >>> eda = AutomatedEDA(df)
    >>> eda.show_summary()
    >>> eda.fill_missing(method_numeric='median')
    >>> eda.detect_outliers(method='zscore', threshold=3)
    >>> normality_df = eda.check_normality()
    """

    def __init__(self, df):
        """
        Initialize the EDA class with a dataframe.

        Parameters
        ----------
        df : pd.DataFrame
            Input dataframe to analyze
        """
        self.df = df.copy()
        self.df_original = df.copy()
        self.preprocessing_log = []
        self._detect_column_types()
        self._log_step("Initialized AutomatedEDA")

    # =========================================================================
    # COLUMN TYPE DETECTION
    # =========================================================================

    def _detect_column_types(self):
        """
        Automatically identify numeric, binary, and categorical columns.

        Classification Rules
        --------------------
        Binary columns:
            - Boolean dtype
            - Numeric with only {0, 1} values
            - Object/category with exactly 2 unique values

        Numeric columns:
            - All numeric dtypes not classified as binary

        Categorical columns:
            - Object/category dtypes with > 2 unique values
        """
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        object_cols = self.df.select_dtypes(include=['object', 'category']).columns.tolist()

        self.binary_cols = []
        self.numeric_cols = []
        self.categorical_cols = []

        # Check numeric columns for binary pattern
        for col in numeric_cols:
            unique_vals = set(self.df[col].dropna().unique())
            if unique_vals.issubset({0, 1}):
                self.binary_cols.append(col)
            else:
                self.numeric_cols.append(col)

        # Check object/category columns
        for col in object_cols:
            unique_vals = self.df[col].dropna().unique()
            if len(unique_vals) == 2:
                self.binary_cols.append(col)
            else:
                self.categorical_cols.append(col)

        # Add boolean dtype directly
        bool_cols = self.df.select_dtypes(include=['bool']).columns.tolist()
        self.binary_cols.extend([col for col in bool_cols if col not in self.binary_cols])

    def show_column_types(self):
        """Display detected column types in a formatted way."""
        print("=" * 70)
        print("DETECTED COLUMN TYPES")
        print("=" * 70)
        print(f"\nNumeric Columns ({len(self.numeric_cols)}):")
        print(f"  {', '.join(self.numeric_cols) if self.numeric_cols else 'None'}")
        print(f"\nBinary Columns ({len(self.binary_cols)}):")
        print(f"  {', '.join(self.binary_cols) if self.binary_cols else 'None'}")
        print(f"\nCategorical Columns ({len(self.categorical_cols)}):")
        print(f"  {', '.join(self.categorical_cols) if self.categorical_cols else 'None'}")
        print("=" * 70)

    # =========================================================================
    # DATA OVERVIEW AND SUMMARY
    # =========================================================================

    def show_summary(self):
        """
        Display comprehensive dataset summary including shape, types,
        missing values, and basic statistics.
        """
        print("\n" + "=" * 70)
        print("DATASET SUMMARY")
        print("=" * 70)
        print(f"Shape: {self.df.shape[0]} rows × {self.df.shape[1]} columns")
        print(f"\nColumn Type Distribution:")
        print(f"  Numeric:     {len(self.numeric_cols)}")
        print(f"  Binary:      {len(self.binary_cols)}")
        print(f"  Categorical: {len(self.categorical_cols)}")

        # Missing values
        missing = self.df.isnull().sum()
        missing_pct = (missing / len(self.df)) * 100
        missing_df = pd.DataFrame({
            'Missing': missing[missing > 0],
            'Percent': missing_pct[missing > 0]
        }).sort_values('Missing', ascending=False)

        if not missing_df.empty:
            print(f"\nMissing Values:")
            print(missing_df.to_string())
        else:
            print(f"\nNo missing values detected.")

        print("=" * 70)

    def plot_distributions(self, figsize=(20, 15)):
        """
        Create comprehensive distribution plots for all variables.

        Parameters
        ----------
        figsize : tuple, default=(20, 15)
            Figure size for the plot grid
        """
        all_cols = self.numeric_cols + self.binary_cols + self.categorical_cols
        n_cols = len(all_cols)

        if n_cols == 0:
            print("No columns to plot.")
            return

        # Calculate grid dimensions
        ncols = 3
        nrows = int(np.ceil(n_cols / ncols))

        fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
        axes = axes.flatten() if n_cols > 1 else [axes]

        for idx, col in enumerate(all_cols):
            ax = axes[idx]

            if col in self.numeric_cols:
                sns.histplot(self.df[col], bins=30, kde=True, ax=ax)
                ax.set_title(f'{col}\n(Numeric)', fontweight='bold')
            elif col in self.binary_cols:
                sns.countplot(x=col, data=self.df, ax=ax)
                ax.set_title(f'{col}\n(Binary)', fontweight='bold')
            else:  # categorical
                value_counts = self.df[col].value_counts()
                if len(value_counts) > 10:
                    value_counts = value_counts.head(10)
                    ax.set_title(f'{col}\n(Categorical - Top 10)', fontweight='bold')
                else:
                    ax.set_title(f'{col}\n(Categorical)', fontweight='bold')
                sns.barplot(x=value_counts.index, y=value_counts.values, ax=ax)
                ax.tick_params(axis='x', rotation=45)

            ax.set_xlabel('')
            ax.grid(True, alpha=0.3)

        # Hide unused subplots
        for idx in range(n_cols, len(axes)):
            axes[idx].axis('off')

        plt.suptitle('Distribution of All Variables', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.show()

    # =========================================================================
    # MISSING VALUE HANDLING
    # =========================================================================

    def fill_missing(self, method_numeric='median', method_binary='mode',
                     method_categorical='mode', knn_cols=None, knn_neighbors=5):
        """
        Fill missing values using specified strategies for each column type.

        Parameters
        ----------
        method_numeric : str, default='median'
            Strategy for numeric columns: 'median', 'mean', or 'none'
        method_binary : str, default='mode'
            Strategy for binary columns: 'mode', 'none', or specific value (0/1)
        method_categorical : str, default='mode'
            Strategy for categorical columns: 'mode', 'none', or 'unknown'
        knn_cols : list, optional
            Columns to impute using KNN imputer
        knn_neighbors : int, default=5
            Number of neighbors for KNN imputation

        Returns
        -------
        self : AutomatedEDA
            Returns self for method chaining
        """
        # Numeric columns
        if method_numeric != 'none':
            for col in self.numeric_cols:
                if self.df[col].isnull().any():
                    if method_numeric == 'median':
                        self.df[col].fillna(self.df[col].median(), inplace=True)
                    elif method_numeric == 'mean':
                        self.df[col].fillna(self.df[col].mean(), inplace=True)
        else print("")

        # Binary columns
        if method_binary != 'none':
            for col in self.binary_cols:
                if self.df[col].isnull().any():
                    if method_binary == 'mode':
                        mode_val = self.df[col].mode()
                        if len(mode_val) > 0:
                            self.df[col].fillna(mode_val[0], inplace=True)
                    elif method_binary in [0, 1]:
                        self.df[col].fillna(method_binary, inplace=True)

        # Categorical columns
        if method_categorical != 'none':
            for col in self.categorical_cols:
                if self.df[col].isnull().any():
                    if method_categorical == 'mode':
                        mode_val = self.df[col].mode()
                        if len(mode_val) > 0:
                            self.df[col].fillna(mode_val[0], inplace=True)
                    elif method_categorical == 'unknown':
                        self.df[col].fillna('Unknown', inplace=True)

        # KNN imputation
        if knn_cols:
            valid_cols = [col for col in knn_cols if col in self.df.columns]
            if valid_cols:
                knn = KNNImputer(n_neighbors=knn_neighbors)
                self.df[valid_cols] = knn.fit_transform(self.df[valid_cols])
                self._log_step(f"KNN imputation applied to {len(valid_cols)} columns")

        self._log_step(f"Missing values filled: numeric={method_numeric}, binary={method_binary}, categorical={method_categorical}")
        return self

    # =========================================================================
    # SCALING AND TRANSFORMATION
    # =========================================================================

    def scale_numeric(self, method='minmax', cols=None):
        """
        Scale numeric columns using specified method.

        Parameters
        ----------
        method : str, default='minmax'
            Scaling method: 'minmax' or 'standard'
        cols : list, optional
            Specific columns to scale. If None, scales all numeric columns

        Returns
        -------
        self : AutomatedEDA
            Returns self for method chaining
        """
        cols_to_scale = cols if cols else self.numeric_cols
        cols_to_scale = [c for c in cols_to_scale if c in self.df.columns]

        if not cols_to_scale:
            print("No valid columns to scale.")
            return self

        if method == 'minmax':
            scaler = MinMaxScaler()
        elif method == 'standard':
            scaler = StandardScaler()
        else:
            raise ValueError("method must be 'minmax' or 'standard'")

        self.df[cols_to_scale] = scaler.fit_transform(self.df[cols_to_scale])
        self._log_step(f"Scaled {len(cols_to_scale)} columns using {method} scaling")
        return self

    def log_transform(self, cols=None, base=10):
        """
        Apply logarithmic transformation to specified columns.

        Parameters
        ----------
        cols : list, optional
            Columns to transform. If None, transforms all numeric columns
        base : int, default=10
            Logarithm base (10 or 'e' for natural log)

        Returns
        -------
        self : AutomatedEDA
            Returns self for method chaining
        """
        cols_to_transform = cols if cols else self.numeric_cols
        cols_to_transform = [c for c in cols_to_transform if c in self.df.columns]

        for col in cols_to_transform:
            if base == 10:
                self.df[col] = np.log10(self.df[col].replace(0, np.nan))
            elif base == 'e':
                self.df[col] = np.log(self.df[col].replace(0, np.nan))
            else:
                self.df[col] = np.log(self.df[col].replace(0, np.nan)) / np.log(base)

            # Fill any resulting NaNs with 0
            self.df[col].fillna(0, inplace=True)

        self._log_step(f"Log{base} transformation applied to {len(cols_to_transform)} columns")
        return self

    # =========================================================================
    # OUTLIER DETECTION
    # =========================================================================

    def detect_outliers(self, method='zscore', threshold=3.0, remove=True, plot=True):
        """
        Detect and optionally remove outliers from numeric columns.

        Parameters
        ----------
        method : str, default='zscore'
            Detection method: 'zscore', 'iqr', or 'isolation_forest'
        threshold : float, default=3.0
            Threshold for outlier detection
            - Z-score: number of standard deviations
            - IQR: multiplier for IQR range
        remove : bool, default=True
            Whether to remove detected outliers
        plot : bool, default=True
            Whether to plot before/after distributions

        Returns
        -------
        pd.DataFrame
            Summary of outliers detected per column
        """
        outlier_report = []
        original_len = len(self.df)
        rows_to_drop = set()

        for col in self.numeric_cols:
            data = self.df[col].dropna()

            if method == 'zscore':
                z_scores = np.abs(stats.zscore(data, nan_policy='omit'))
                mask = z_scores > threshold
            elif method == 'iqr':
                Q1, Q3 = np.percentile(data, [25, 75])
                IQR = Q3 - Q1
                mask = (data < Q1 - threshold * IQR) | (data > Q3 + threshold * IQR)
            else:
                raise ValueError("method must be 'zscore' or 'iqr'")

            outlier_indices = data[mask].index
            rows_to_drop.update(outlier_indices)

            n_outliers = mask.sum()
            pct = (n_outliers / len(data)) * 100
            outlier_report.append([col, n_outliers, len(data), pct])

            if plot and n_outliers > 0:
                fig, axes = plt.subplots(1, 2, figsize=(12, 4))

                # Before
                sns.histplot(self.df_original[col], bins=30, kde=True, ax=axes[0])
                axes[0].set_title(f'{col} - Before Outlier Removal\n(n={len(self.df_original)})',
                                 fontweight='bold')
                axes[0].set_xlabel(col)

                # After
                clean_data = self.df[col][~self.df.index.isin(outlier_indices)]
                sns.histplot(clean_data, bins=30, kde=True, ax=axes[1], color='green')
                axes[1].set_title(f'{col} - After Outlier Removal\n(n={len(clean_data)}, removed={n_outliers})',
                                 fontweight='bold')
                axes[1].set_xlabel(col)

                plt.tight_layout()
                plt.show()

        if remove and rows_to_drop:
            self.df = self.df.drop(index=list(rows_to_drop))
            self.df.reset_index(drop=True, inplace=True)
            total_removed = len(rows_to_drop)
            pct_removed = (total_removed / original_len) * 100
            self._log_step(f"Removed {total_removed} rows ({pct_removed:.2f}%) containing outliers")

        report_df = pd.DataFrame(outlier_report,
                                columns=['Variable', 'Outliers', 'Total', 'Percent'])
        return report_df

    # =========================================================================
    # NORMALITY TESTING
    # =========================================================================

    def check_normality(self, plot=True):
        """
        Perform comprehensive normality tests on numeric columns.

        Tests performed:
        - Shapiro-Wilk test
        - D'Agostino-Pearson K² test
        - Skewness coefficient
        - Kurtosis coefficient

        Parameters
        ----------
        plot : bool, default=True
            Whether to create Q-Q plots for each variable

        Returns
        -------
        pd.DataFrame
            Summary of normality tests for all numeric columns
        """
        normality_results = []

        for col in self.numeric_cols:
            data = self.df[col].dropna()

            if len(data) < 3:
                continue

            # Shapiro-Wilk test (best for n < 5000)
            if len(data) < 5000:
                stat_sw, p_sw = stats.shapiro(data)
            else:
                stat_sw, p_sw = np.nan, np.nan

            # D'Agostino-Pearson test (better for large samples)
            try:
                stat_k2, p_k2 = stats.normaltest(data)
            except:
                stat_k2, p_k2 = np.nan, np.nan

            # Skewness and Kurtosis
            skew = stats.skew(data)
            kurt = stats.kurtosis(data)

            normality_results.append([col, stat_sw, p_sw, stat_k2, p_k2, skew, kurt])

            # Q-Q plot
            if plot:
                fig, axes = plt.subplots(1, 2, figsize=(12, 4))

                # Histogram with normal curve overlay
                sns.histplot(data, bins=30, kde=True, stat='density', ax=axes[0])
                mu, sigma = data.mean(), data.std()
                x = np.linspace(data.min(), data.max(), 100)
                axes[0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-',
                           label='Normal Distribution', linewidth=2)
                axes[0].set_title(f'{col} - Distribution', fontweight='bold')
                axes[0].legend()

                # Q-Q plot
                stats.probplot(data, dist="norm", plot=axes[1])
                axes[1].set_title(f'{col} - Q-Q Plot', fontweight='bold')
                axes[1].grid(True, alpha=0.3)

                plt.tight_layout()
                plt.show()

        results_df = pd.DataFrame(
            normality_results,
            columns=['Variable', 'Shapiro_Stat', 'Shapiro_p',
                    'DAgostino_Stat', 'DAgostino_p', 'Skewness', 'Kurtosis']
        )

        return results_df

    # =========================================================================
    # ENCODING
    # =========================================================================

    def encode_categorical(self, ordinal_mappings=None, onehot_cols=None, drop_first=True):
        """
        Encode categorical and binary columns.

        Parameters
        ----------
        ordinal_mappings : dict, optional
            Dictionary mapping column names to ordered categories
            Example: {'severity': ['Low', 'Medium', 'High']}
        onehot_cols : list, optional
            Specific columns for one-hot encoding. If None, encodes all categorical columns
        drop_first : bool, default=True
            Whether to drop first category in one-hot encoding (avoid dummy trap)

        Returns
        -------
        self : AutomatedEDA
            Returns self for method chaining
        """
        # Ordinal encoding
        if ordinal_mappings:
            for col, order in ordinal_mappings.items():
                if col in self.df.columns:
                    enc = OrdinalEncoder(categories=[order])
                    self.df[col] = enc.fit_transform(self.df[[col]])
                    self._log_step(f"Ordinal encoding applied to '{col}'")

        # Binary string columns to 0/1
        for col in self.binary_cols:
            if self.df[col].dtype == 'object':
                unique_vals = self.df[col].dropna().unique()
                if len(unique_vals) == 2:
                    mapping = {unique_vals[0]: 0, unique_vals[1]: 1}
                    self.df[col] = self.df[col].map(mapping)
                    self._log_step(f"Binary encoding applied to '{col}': {mapping}")

        # One-hot encoding
        cols_to_encode = onehot_cols if onehot_cols else self.categorical_cols
        cols_to_encode = [c for c in cols_to_encode
                         if c in self.df.columns and c not in (ordinal_mappings or {}).keys()]

        if cols_to_encode:
            self.df = pd.get_dummies(self.df, columns=cols_to_encode, drop_first=drop_first)
            self._log_step(f"One-hot encoding applied to {len(cols_to_encode)} columns")
            # Update column lists after encoding
            self._detect_column_types()

        return self

    # =========================================================================
    # CLASS IMBALANCE
    # =========================================================================

    def report_imbalance(self, target_cols=None, plot=True):
        """
        Analyze and report class imbalance in binary/categorical columns.

        Parameters
        ----------
        target_cols : list, optional
            Columns to analyze. If None, analyzes all binary and categorical columns
        plot : bool, default=True
            Whether to create distribution plots

        Returns
        -------
        dict
            Dictionary mapping column names to DataFrames with count and percentage
        """
        if target_cols is None:
            target_cols = self.binary_cols + self.categorical_cols

        target_cols = [c for c in target_cols if c in self.df.columns]
        imbalance_report = {}

        for col in target_cols:
            counts = self.df[col].value_counts()
            percentages = self.df[col].value_counts(normalize=True) * 100

            report_df = pd.concat([counts, percentages], axis=1, keys=['Count', 'Percent'])
            imbalance_report[col] = report_df

            print(f"\n{col}:")
            print(report_df.to_string())

            if plot:
                plt.figure(figsize=(8, 4))
                ax = sns.countplot(x=col, data=self.df, order=counts.index)
                plt.title(f'Distribution of {col}', fontweight='bold', fontsize=12)
                plt.xlabel(col)
                plt.ylabel('Count')

                # Add percentage labels on bars
                total = len(self.df[col].dropna())
                for p in ax.patches:
                    height = p.get_height()
                    ax.text(p.get_x() + p.get_width()/2., height + total*0.01,
                           f'{height/total*100:.1f}%',
                           ha="center", fontsize=10)

                plt.grid(True, alpha=0.3, axis='y')
                plt.tight_layout()
                plt.show()

        return imbalance_report

    def resample(self, target_col, method='smote', sampling_strategy='auto', random_state=42):
        """
        Resample dataset to handle class imbalance.

        Parameters
        ----------
        target_col : str
            Target column name
        method : str, default='smote'
            Resampling method: 'smote' or 'undersample'
        sampling_strategy : str or dict, default='auto'
            Sampling strategy (see imblearn documentation)
        random_state : int, default=42
            Random state for reproducibility

        Returns
        -------
        pd.DataFrame
            Resampled dataframe
        """
        if target_col not in self.df.columns:
            raise ValueError(f"Target column '{target_col}' not found")

        X = self.df.drop(columns=target_col)
        y = self.df[target_col]

        if method == 'smote':
            sampler = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)
        elif method == 'undersample':
            sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=random_state)
        else:
            raise ValueError("method must be 'smote' or 'undersample'")

        X_res, y_res = sampler.fit_resample(X, y)
        self.df = pd.concat([X_res, y_res], axis=1)

        self._log_step(f"Resampled using {method}: {len(self.df_original)} → {len(self.df)} rows")
        return self.df

    # =========================================================================
    # UTILITY METHODS
    # =========================================================================

    def _log_step(self, message):
        """Log a preprocessing step."""
        self.preprocessing_log.append(message)

    def show_log(self):
        """Display preprocessing log."""
        print("\n" + "=" * 70)
        print("PREPROCESSING LOG")
        print("=" * 70)
        for i, step in enumerate(self.preprocessing_log, 1):
            print(f"{i}. {step}")
        print("=" * 70)

    def reset(self):
        """Reset dataframe to original state."""
        self.df = self.df_original.copy()
        self.preprocessing_log = ["Reset to original dataframe"]
        self._detect_column_types()
        return self

    def get_processed_df(self):
        """Return the processed dataframe."""
        return self.df.copy()